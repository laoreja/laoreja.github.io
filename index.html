<!DOCTYPE html>
<html>
<head>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-88380739-1', 'auto');
      ga('send', 'pageview');
    </script>	

    <!-- GoStats JavaScript Based Code -->
	<script type="text/javascript" src="https://ssl.gostats.com/js/counter.js"></script>
	<script type="text/javascript">_gos='monster.gostats.com';_goa=488062;_got=5;_goi=1;_gol='website page counter';_GoStatsRun();
	</script>
	<noscript>
		<a target="_blank" title="website page counter" href="http://gostats.com"><img alt="website page counter" src="https://ssl.gostats.com/bin/count/a_488062/t_5/i_1/ssl_monster.gostats.com/counter.png" style="border-width:0" />
		</a>
	</noscript>
<!-- End GoStats JavaScript Based Code -->

<title>Xiuye's Home</title>
<!-- for-mobile-apps -->
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="keywords" content="Xiuye Gu, Computer Vision, Machine Learning, Research, Personal Website" />
<script type="application/x-javascript"> addEventListener("load", function() { setTimeout(hideURLbar, 0); }, false);function hideURLbar(){ window.scrollTo(0,1); } </script>
<!-- //for-mobile-apps -->
<link href="css/bootstrap.css" rel="stylesheet" type="text/css" media="all" />
<link href="css/style.css" rel="stylesheet" type="text/css" media="all" />
<!--fonts-->
<link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
<link href='https://fonts.googleapis.com/css?family=Berkshire+Swash' rel='stylesheet' type='text/css'>
<!--/fonts-->
<!-- js -->
<script src="js/jquery.min.js"> </script>
<script src="js/bootstrap.js"></script>
<!-- //js -->
<!-- start-smoth-scrolling -->
<script type="text/javascript" src="js/move-top.js"></script>
<script type="text/javascript" src="js/easing.js"></script>
<script type="text/javascript">
	jQuery(document).ready(function($) {
		$(".scroll").click(function(event){		
			event.preventDefault();
			$('html,body').animate({scrollTop:$(this.hash).offset().top},1000);
		});
	});
</script>
<link rel="shortcut icon" type="image/png" href="images/favicon.png">
<!-- start-smoth-scrolling -->
</head>
<body>


<div class="space"></div>
<div class="space"></div>
<div class="space"></div>
<!-- COMMON HEAD END -->


<div class="container">
	<div class="head-section text-center">
		<div class="space"></div>
		<h2>XIUYE GU</h2>
		<span> </span>
	</div>

	<div class="my_photo me">
		<img class="my_img" src="images/IMG-2877.jpg">
	</div>
	<div class="self_intro me">
		<p>
			Hi, I'm a researcher at <a href="https://deepmind.google/">Google DeepMind</a>. Before that, I was a Google AI Resident, where I was lucky to be advised by <a href="https://ycui.me/">Yin Cui</a> and <a href="https://scholar.google.com/citations?user=_BPdgV0AAAAJ&hl=en">Tsung-Yi Lin</a> and worked on open-vocabulary visual recognition. Previously, I obtained my M.S. in Computer Science at <a href="https://www.stanford.edu/">Stanford University</a>, and my B.E. in Computer Science at Zhejiang University. I spent half a year (9/2018-3/2019) and summer 2016 working happily with <a href="http://www.cs.ucdavis.edu/~yjlee/">Prof. Yong Jae Lee</a> at UC Davis. In summer 2018, I interned at <a href="https://www.tusimple.com/">TuSimple</a> and worked on 3D point cloud scene flow estimation with <a href="http://acsweb.ucsd.edu/~pawang/homepage_PhD/"><nobr>Dr. Panqu Wang</nobr></a>. At Zhejiang University, I was advised by <a href="http://www.cad.zju.edu.cn/home/dengcai/">Prof. Deng Cai</a>. 
		</p>
		<p style="margin-bottom:1.5em">
			<!-- My primary research interest is Computer Vision.  -->
			Currently, I'm interested into video generation and open-vocabulary visual recognition.
			<!-- More specifically, I&apos;m interested in improving machines&apos; visual understanding through taking multi-modal inputs and better modeling. -->
			<!-- I am interested in Computer Vision and Machine Learning. I want to solve high level vision problems, and strive to gain a deeper understanding of Deep Learning. Besides, I am also interested in Approximate Nearest Neighbor Search, in the field of Information Retrieval. -->
		</p>
		<p style="margin-bottom:1.5em">			
		<a href="mailto:laoreja0922@gmail.com"> <img src="images/Email.png" width="8%" hspace="2%"> </a>
		<a href="https://github.com/laoreja"> <img src="images/github-logo.svg" width="8%" hspace="2%"> </a>
		<a href="https://scholar.google.com/citations?user=qCrypnoAAAAJ&hl=en"> <img src="images/google-scholar-foreground.svg" width="8%" hspace="2%"> </a> 
		<a href="https://twitter.com/laoreja001"> <img src="images/Twitter_bird_logo_black.svg" width="8%" hspace="2%"> </a>
		<a href="https://www.linkedin.com/in/xiuyegu/"> <img src="images/linkedin-logo.png" width="8%" hspace="2%"> </a>
		</p>

<!-- 		<p style="margin-bottom:1.5em">			
		<a href="mailto:laoreja0922@gmail.com"> <img src="images/Email.png" width="25"> </a> <a href="mailto:laoreja0922@gmail.com">laoreja0922 at gmail dot com </a></p>
		<p><a href="https://github.com/laoreja"> <img src="images/GitHub.png" width="25"> </a> <a href="https://github.com/laoreja">https://github.com/laoreja </a>
		<p><a href="https://scholar.google.com/citations?user=qCrypnoAAAAJ&hl=en"> <img src="images/google-scholar-square.svg" width="25"> </a> <a href="https://scholar.google.com/citations?user=qCrypnoAAAAJ&hl=en">https://scholar.google.com/citations?user=qCrypnoAAAAJ&hl=en </a>
		<p><a href="https://twitter.com/laoreja001"> <img src="images/twitter-logo.png" width="25"> </a> <a href="https://twitter.com/laoreja001">https://twitter.com/laoreja001 </a>
		<p><a href="https://www.linkedin.com/in/xiuyegu/"> <img src="images/linkedin-logo.png" width="25"> </a> <a href="https://www.linkedin.com/in/xiuyegu/">https://www.linkedin.com/in/xiuyegu/ </a>
		</p> -->
	</div>
</div>



<div class="container"><hr style="border-top: 1px solid #D8D8D8"></div>
<div class="space"></div>



<div class="publications" id="research">
	<div class="container">
		<div class="head-section text-center">
			<h2>Publications</h2>
			<span> </span>
		</div>
		<div class="projects_table" id="research_table">
        	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
				<tbody>
					<tr>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./paper_teasers/textok_teaser.png" width="100%"></div>
			      </td>
						<td valign="top" width="75%">
			        		<p><papertitle>Kaiwen Zha, Lijun Yu, Alireza Fathi, David Ross, Cordelia Schmid, Dina Katabi, <b>Xiuye Gu</b>.<br> <em>Language-Guided Image Tokenization for Generation (<b>TexTok</b>).</em><br> CVPR 2025.</papertitle></p>
			        		<br>
			        		<p>
			        			<a href="https://arxiv.org/abs/2412.05796">[paper]</a>
			        		</p>		        	
						</td>
					</tr>						
					<tr>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./paper_teasers/video_poet_teaser.png" width="100%"></div>
			      </td>
						<td valign="top" width="75%">
			        		<p><papertitle>Dan Kondratyuk*, Lijun Yu*, <b>Xiuye Gu*</b>, Jos√© Lezama*, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, Yong Cheng, Ming-Chang Chiu, Josh Dillon, Irfan Essa, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, David Ross, Grant Schindler, Mikhail Sirotenko, Kihyuk Sohn, Krishna Somandepalli, Huisheng Wang, Jimmy Yan, Ming-Hsuan Yang, Xuan Yang, Bryan Seybold, Lu Jiang.<br> <em><b>VideoPoet</b>: A large language model for zero-shot video generation.</em><br> ICML 2024, <b style="color:red;">Best Paper Award, Patent</b>.</papertitle></p>
			        		<br>
			        		<p>
			        			<a href="https://arxiv.org/abs/2312.14125">[paper]</a> &nbsp; 
			        			<a href="https://sites.research.google/videopoet/">[website with demos]</a> &nbsp; 
			        			<a href="https://blog.research.google/2023/12/videopoet-large-language-model-for-zero.html">[blog]</a>
			        		</p>		        	
						</td>
					</tr>
					<tr>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./paper_teasers/pixel_llm_teaser.gif" width="100%"></div>
			      </td>
						<td valign="top" width="75%">
			        		<p><papertitle>Jiarui Xu, Xingyi Zhou, Shen Yan, <b>Xiuye Gu</b>, Anurag Arnab, Chen Sun, Xiaolong Wang, Cordelia Schmid.<br> <em>Pixel-Aligned Language Model.</em><br>CVPR 2024.</papertitle></p>
			        		<br>
			        		<p>
			        			<a href="https://arxiv.org/abs/2312.09237">[paper]</a> &nbsp; 
			        			<a href="https://github.com/google-research/scenic/tree/main/scenic/projects/pixel_llm">[code]</a>
			        		</p>		        	
						</td>
					</tr>												
					<tr>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./paper_teasers/car_teaser.png" width="100%"></div>
			      </td>
						<td valign="top" width="75%">
			        		<p><papertitle>Shuyang Sun, Runjia Li, Philip Torr, <b>Xiuye Gu*</b>, Siyang Li*.<br> <em><b>CLIP as RNN</b>: Segment Countless Visual Concepts without Training Endeavor.</em><br>  CVPR 2024.</papertitle></p>
			        		<br>
			        		<p>
			        			<a href="https://arxiv.org/abs/2312.07661">[paper]</a> &nbsp; 
			        			<a href="https://github.com/kevin-ssy/CLIP_as_RNN">[code]</a> &nbsp; 
			        			<a href="https://torrvision.com/clip_as_rnn/">[website]</a> 
			        		</p>		        	
						</td>
					</tr>						
					<tr>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./paper_teasers/walt_teaser.png" width="100%"></div>
			      </td>
						<td valign="top" width="75%">
			        		<p><papertitle>Agrim Gupta, Lijun Yu, Kihyuk Sohn, <b>Xiuye Gu</b>, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, Jos√© Lezama.<br> <em>Photorealistic video generation with diffusion models (<b>W.A.L.T</b>).</em><br>  ECCV 2024.</papertitle></p>
			        		<br>
			        		<p>
			        			<a href="https://arxiv.org/abs/2312.06662">[paper]</a> &nbsp; 
			        			<a href="https://walt-video-diffusion.github.io/">[website]</a> &nbsp;
			        			<a href="https://youtu.be/ek0U8iJ8j3c">[video demo]</a> &nbsp;
			        			<a href="https://walt-video-diffusion.github.io/samples.html">[more samples]</a>
			        		</p>		        	
						</td>
					</tr>					
					<tr>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./paper_teasers/magvit_v2_teaser.png" width="100%"></div>
			      </td>
						<td valign="top" width="75%">
			        		<p><papertitle>Lijun Yu, Jos√© Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, <b>Xiuye Gu</b>, Alexander G Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A Ross, Lu Jiang.<br> <em>Language Model Beats Diffusion--Tokenizer is Key to Visual Generation.</em><br>  ICLR 2024.</papertitle></p>
			        		<br>
			        		<p>
			        			<a href="https://arxiv.org/abs/2310.05737">[paper]</a> &nbsp; 
			        			<a href="https://magvit.cs.cmu.edu/v2/">[website]</a>
			        		</p>		        	
						</td>
					</tr>						
					<tr>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./paper_teasers/dataseg_teaser.png" width="100%"></div>
			      </td>
						<td valign="top" width="75%">
			        		<p><papertitle><b>Xiuye Gu</b>, Yin Cui, Jonathan Huang, Abdullah Rashwan, Xuan Yang, Xingyi Zhou, Golnaz Ghiasi, Weicheng Kuo, Huizhong Chen, Liang-Chieh Chen, David A Ross.<br> <em><b>DaTaSeg</b>: Taming a Universal Multi-Dataset Multi-Task Segmentation Model.</em><br> NeurIPS 2023.</papertitle></p>
			        		<br>
			        		<p>
			        			<a href="https://arxiv.org/abs/2306.01736">[paper]</a> &nbsp; 
			        			<a href="https://github.com/google-research-datasets/DaTaSeg-Objects365-Instance-Segmentation">[Objects365 instance segmentation dataset]</a>  &nbsp; 
			        			<a href="./paper_teasers/dataseg_poster.pdf">[poster]</a>
			        		</p>		        	
						</td>
					</tr>		
					<tr>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./paper_teasers/polymax_teaser.png" width="100%"></div>
			      </td>
						<td valign="top" width="75%">
			        		<p><papertitle>Xuan Yang, Liangzhe Yuan, Kimberly Wilber, Astuti Sharma, <b>Xiuye Gu</b>, Siyuan Qiao, Stephanie Debats, Huisheng Wang, Hartwig Adam, Mikhail Sirotenko, Liang-Chieh Chen.<br> <em><b>PolyMaX</b>: General Dense Prediction with Mask Transformer.</em><br>  WACV 2024.</papertitle></p>
			        		<br>
			        		<p>
			        			<a href="https://openaccess.thecvf.com/content/WACV2024/papers/Yang_PolyMaX_General_Dense_Prediction_With_Mask_Transformer_WACV_2024_paper.pdf">[paper]</a> &nbsp; 
			        			<a href="https://openaccess.thecvf.com/content/WACV2024/supplemental/Yang_PolyMaX_General_Dense_WACV_2024_supplemental.pdf">[supp]</a>
			        		</p>		        	
						</td>
					</tr>									
					<tr>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./paper_teasers/zpe_teaser.png" width="100%"></div>
			      </td>
						<td valign="top" width="75%">
			        		<p><papertitle>James Urquhart Allingham, Jie Ren, Michael W Dusenberry, Jeremiah Zhe Liu, <b>Xiuye Gu</b>, Yin Cui, Dustin Tran, Balaji Lakshminarayanan.<br> <em>A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models.</em><br>ICML 2023, <b>Patent</b>.</papertitle></p>
			        		<br>
			        		<p>
			        			<a href="https://arxiv.org/abs/2302.06235">[paper]</a>
			        		</p>		        	
						</td>
					</tr>					
					<tr>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./paper_teasers/F-VLM.png" width="100%"></div>
			      </td>
						<td valign="top" width="75%">
			        		<p><papertitle>Weicheng Kuo, Yin Cui, <b>Xiuye Gu</b>, AJ Piergiovanni, Anelia Angelova.<br> <em><b>F-vlm</b>: Open-vocabulary object detection upon frozen vision and language models.</em><br> ICLR 2023, <b>Patent</b>.</papertitle></p>
			        		<br>
			        		<p>
			        			<a href="https://arxiv.org/abs/2209.15639">[paper]</a> &nbsp; 
			        			<a href="https://github.com/google-research/google-research/tree/master/fvlm">[code]</a> &nbsp; 
			        			<a href="https://sites.google.com/corp/view/f-vlm/home">[website]</a> &nbsp; 
			        			<a href="http://goto.google.com/fvlm-blog">[blog]</a>
			        		</p>		        	
						</td>
					</tr>
					<tr>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./paper_teasers/openseg_teaser.png" width="100%"></div>
			      </td>
						<td valign="top" width="75%">
			        		<p><papertitle>Golnaz Ghiasi, <b>Xiuye Gu</b>, Yin Cui, Tsung-Yi Lin.<br> <em>Scaling open-vocabulary image segmentation with image-level labels (<b>OpenSeg</b>).</em><br> ECCV 2022.</papertitle></p>
			        		<br>
			        		<p>
			        			<a href="https://arxiv.org/abs/2112.12143">[paper]</a> &nbsp; 
			        			<a href="https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/openseg">[code]</a> &nbsp; 
			        			<a href="https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/models/official/detection/projects/openseg/OpenSeg_demo.ipynb">[colab demo]</a> &nbsp; 
			        			<a href="https://storage.googleapis.com/cloud-tpu-checkpoints/detection/projects/openseg/OpenSeg-ECCV22-poster.pdf">[poster]</a>
			        		</p>		        	
						</td>
					</tr>
					<tr>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./paper_teasers/vild_teaser.png" width="100%"></div>
			      </td>
						<td valign="top" width="75%">
			        		<p><papertitle><b>Xiuye Gu</b>, Tsung-Yi Lin, Weicheng Kuo, Yin Cui.<br> <em>Open-vocabulary object detection via vision and language knowledge distillation (<b>ViLD</b>).</em><br> ICLR 2022.</papertitle></p>
			        		<br>
			        		<p>
			        			<a href="https://arxiv.org/abs/2104.13921">[paper]</a> &nbsp; 
			        			<a href="https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild">[code]</a> &nbsp; 
			        			<a href="https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/models/official/detection/projects/vild/ViLD_demo.ipynb">[colab demo]</a>
			        		</p>		        	
						</td>
					</tr>					
					<tr>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./paper_teasers/face_transformer_teaser.png" width="100%"></div>
			          	</td>
						<!-- <td width="25%">
		            		<div class="two" id="ccc_image"><img src="./paper_teasers/face_transformer_teaser.png" width="250px" height="250px" object-fit="cover"></div>
			          	</td> -->
						<td valign="top" width="75%">
			        		<p><papertitle><b>Xiuye Gu</b>, Weixin Luo, Michael S. Ryoo, Yong Jae Lee.<br> <em>Password-conditioned Anonymization and Deanonymization with Face Identity Transformers.</em><br> ECCV 2020.</papertitle></p>
			        		<br>
			        		<p>
			        			<a href="https://arxiv.org/abs/1911.11759">[paper]</a> &nbsp; 
			        			<!-- <a href="./papers/face_transformer_poster.pdf">[poster]</a> &nbsp;  -->
			        			<a href="https://github.com/laoreja/face-identity-transformer">[code]</a> &nbsp; 
			        			<a href="https://www.youtube.com/watch?v=FrYmf-CL4yk&t=2s">[demo video]</a> <br>
			        			<a href="https://youtu.be/9nn3lvr6IsU">[1-min presentation]</a> &nbsp; 
			        			<a href="https://youtu.be/hANVFxC9Szo">[10-min presentation]</a>
			        		</p>		        	
						</td>
					</tr>					
					<tr>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./paper_teasers/hplflownet_w_net.png" width="100%"></div>
			          	</td>						
						<!-- <td width="25%">
		            		<div class="two" id="ccc_image"><img src="./paper_teasers/hplflownet.png" width="250px" height="250px" object-fit="cover"></div>
			          	</td> -->
						<td valign="top" width="75%">
			        		<p><papertitle><b>Xiuye Gu</b>, Yijie Wang, Chongruo Wu, Panqu Wang, Yong Jae Lee.<br> <em>HPLFlowNet: Hierarchical Permutohedral Lattice FlowNet for Scene Flow Estimation on Large-scale Point Clouds.</em><br> CVPR 2019.</papertitle></p>
			        		<br>
			        		<p>
			        			<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Gu_HPLFlowNet_Hierarchical_Permutohedral_Lattice_FlowNet_for_Scene_Flow_Estimation_on_CVPR_2019_paper.pdf">[paper]</a> &nbsp; 
			        			<a href="https://web.cs.ucdavis.edu/~yjlee/projects/cvpr2019-HPLFlowNet-supp.pdf">[supp]</a> &nbsp; 
			        			<a href="https://github.com/laoreja/HPLFlowNet">[code]</a> &nbsp; 
			        			<a href="./papers/hplflownet_poster.pdf">[poster]</a> &nbsp; 
			        			<a href="./papers/cvpr2019_video.mp4">[video]</a>
			        		</p>		        	
						</td>
					</tr>
					<tr>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./paper_teasers/horse_hq.png" width="100%"></div>
			          	</td>
						<!-- <td width="25%">
		            		<div class="two" id="ccc_image"><img src="./paper_teasers/horse_hq.png" width="250px" height="250px" object-fit="cover"></div>
			          	</td> -->
						<td valign="top" width="75%">
			        		<p><papertitle>Maheen Rashid, <b>Xiuye Gu</b>, Yong Jae Lee. <br><em>Interspecies Knowledge Transfer for Facial Keypoint Detection.</em><br> CVPR 2017.</papertitle></p>
			        		<br>
			        		<p>
			        			<a href="https://web.cs.ucdavis.edu/~yjlee/projects/interspecies_cvpr2017.pdf">[paper]</a> &nbsp; 
			        			<a href="https://github.com/menoRashid/animal_human_kp">[code]</a> &nbsp; 
								<a href="https://github.com/laoreja/facial-landmark-annotation-tools">[anotation tool]</a>
			        		</p>		        	
						</td>
					</tr>
<!-- 					<tr>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./paper_teasers/revisit_new.png" width="100%"></div>
			          	</td>						
						<td valign="top" width="75%">
							<p>
							<papertitle>Deng Cai, <b>Xiuye Gu</b>, Chaoqi Wang.<br> <em>A Revisit on Deep Hashing for Large-scale Content Based Image Retrieval.</em><br> arXiv:1711.06016, 2017.</papertitle>
							</p>
							<br>
							<p><a href="https://arxiv.org/abs/1711.06016">[paper]</a></p>
							</p>
						</td>
					</tr> -->
				</tbody>
			</table>
		</div>
	</div>
</div>	

<div class="space"></div>
<div class="space"></div>
<!-- <div class="space"></div> -->

<div class="container">
	<div class="head-section text-center">
		<h2>Community Services</h2>
		<span> </span>
	</div>
	<div class="space"></div>
	<div>
		<p>
			<ul>
				<li>Organizing committee of <a href="https://computer-vision-in-the-wild.github.io/cvpr-2023/">2nd Computer Vision in the Wild (CVinW) workshop </a> at CVPR 2023.</li>
				<li>Invited speaker at Adobe Research GenTech Seminar, 2024.</li>
				<li>Hosted a demo session at <a href="https://research.google/blog/google-at-neurips-2023/">Google Research booth</a>, NeurIPS 2023.</li>
				<li>Invited speaker at <a href="https://l2id.github.io/l2id2022/">Learning from Limited and Imperfect Data (L2ID) workshop</a> at ECCV 2022.</li>
			  <li>Reviewer of IJCV, TIP, CVPR, ECCV, ICCV, WACV, 3DV, AAAI, NeurIPS, ICML, IROS.</li>
			  <li>Volunteer work: Mentored high school students from historically underrepresented groups at <a href="https://ai-4-all.org/">AI4ALL</a> 2022.</li>
			</ul>
		</p>
	</div>
</div>

<div class="space"></div>
<div class="container"><hr style="border-top: 1px solid #D8D8D8"></div>
<div class="space"></div>


<div class="publications" id="research">
	<div class="container">
		<div class="head-section text-center">
			<h2>Projects</h2>
			<span> </span>
		</div>
		<div class="projects_table" id="research_table">
        	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
				<tbody>
					<tr>
						<td valign="top" width="75%">
			        		<p><papertitle><b>Explore Deep Graph Generation</b></papertitle></p>
			        		<br>
			        		<p>Course project for <a href="http://web.stanford.edu/class/cs224w/">CS224W: Machine Learning with Graphs</a>.<br>
			        		Explore deep graph generation from two directions:<br />
							1) use CNN GANs to model the whole adjacency matrix
							directly after sorting the nodes;<br>
							2) build upon the very recent Graph Recurrent Attention
							Networks (GRANs), proposed a graph completeness judger
							network and improved its attention mechanism.<br>
							<!-- For the second direction, experiments on the Grid and
							Protein datasets show that our improved version
							outperforms the original approach and the completeness
							judger is effective. -->
							</p>
			        		<p>
			        			<a href="./projects/cs224w_poster.pdf">[poster]</a> &nbsp; 
			        			<a href="./projects/cs224w_report.pdf">[report]</a> &nbsp; 
			        			<a href="https://github.com/laoreja/CS224W-project-Deep-graph-generation">[code]</a>
			        		</p>
			        		<!-- <ul>
			        			<li><p><b>Background:</b> Automatic car license plate recognition systems can be very helpful in transportation management systems. However, current work on this topic still cannot reach a very high accuracy.</p></li>
			        			<li><p><b>Approach:</b> In this project, by proposing an iterative segmentation algorithm and critically integrating ideas derived from related research, I successfully achieved an error rate of 4% on License Plate Segementation. To boost the segmentation performance, I incorporated more robust slant and skew correction in the detection stage. I made my system robust when the plates were incomplete, tilted, skewed, blurred by spots, or with poor illumination.<a href="https://github.com/laoreja/LicensePlate">[code]</a></p></li>
			        			<li><p><b>Miscellaneous:</b> I also wrote three literature reviews on license plate detection, segmentation, and character recognition.</p></li>
			        		</ul> -->
						</td>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./projects/cs224w_teaser_shorter.png" width="100%"></div>
			          	</td>
					</tr>
					<tr>
						<td valign="top" width="75%">
			        		<p><papertitle><b>Simulating and Rendering Explosion</b></papertitle></p>
			        		<br>
			        		<p>Course project for <a href="http://graphics.stanford.edu/courses/cs348b/">CS348B: Image Synthesis Techniques</a>.<br>
			        			Extended PBRT to support emissive volumes and openVDB input format.<br>
			        			Self-studied and implemented blackbody radiation, closed-form and delta tracking.<br>
			        			Simulated explosion and Ô¨Çying rubbles using Blender.<br>
			        		</p>

			        		<p>
			        			<a href="./projects/cs348b_report.pdf">[report]</a> &nbsp; 
								<a href="https://youtu.be/xADJ_W4vA_0">[video]</a> &nbsp; 			        			
			        			<a href="https://github.com/laoreja/CS348b-project-Explosion">[code]</a>
			        		</p>
						</td>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="https://github.com/laoreja/image_bed/raw/master/ezgif-1-7033462e45f1.gif" width="100%"></div>
			          	</td>
					</tr>					
					<tr>
						<td valign="top" width="75%">
			        		<p><papertitle><b>License Plate Detection and Character Segmentation</b></papertitle></p>
			        		<br>
			        		<p>A robust iterative license plate character segmentation algorithm and a license detection system with robust skew and slant correction to improve character segmentation.</p>
			        		<p>
				        		<a href="./projects/chracter_segmentation.pdf">[character segmentation report]</a> &nbsp; 
				        		<a href="./projects/detection.pdf">[detection report]</a> <br /> 
				        		<a href="./projects/detection_survey.pdf">[detection survey]</a> &nbsp; 	
				        		<a href="./projects/segmentation_survey.pdf">[segmentation survey]</a> &nbsp; 	
				        		<a href="./projects/recognition_survey.pdf">[recognition survey]</a> <br /> 
				        		<a href="https://github.com/laoreja/LicensePlate">[code]</a> &nbsp; (reports and surveys are in Chinese)
			        		</p>
						</td>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./images/plate.png" width="100%"></div>
			          	</td>
					</tr>
					<tr>
						<td valign="top" width="75%">
			        		<p><papertitle><b>Deep Stereo Matching</b></papertitle></p>
			        		<br>
			        		<p>Course project for <a href="http://web.stanford.edu/class/cs231a/">CS231A: Computer Vision, From 3D Reconstruction to Recognition</a>.</p>
			        		<p>
			        			<a href="https://github.com/laoreja/CS231A-project-Stereo-matching">[code]</a>  &nbsp; 
			        			<a href="./projects/CS231A_report.pdf">[report]</a> &nbsp; 
			        			<a href="./projects/CS231A_supp.pdf">[supp]</a>
			        		</p>

						</td>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./projects/CS231A_teaser_new.png" width="100%"></div>
			          	</td>
					</tr>
<!-- 					<tr>
						<td valign="top" width="75%">
			        		<p><papertitle><b>Speedup Pointnet++</b></papertitle></p>
			        		<br>
			        		<p>Various techniques to speed up the <a href="http://stanford.edu/~rqi/pointnet2/">Pointnet++</a> implementation.</p>
			        		<p>
			        			<a href="./projects/pointnet2_report.pdf">[report]</a> &nbsp; 
			        			<a href="https://github.com/laoreja/Speedup-Pointnet2">[code]</a>
			        		</p>

						</td>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./projects/pointnet2_arch.png" width="100%"></div>
			          	</td>
					</tr>
					<tr>
						<td valign="top" width="75%">
			        		<p><papertitle><b>Human or Robot?</b></papertitle></p>
			        		<br>
			        		<p>Course project for <a href="http://cs229.stanford.edu/">CS229: Machine Learning</a>.<br> Applied the machine learning best practice learned in the class to the Kaggle task <a href="https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot/">Human or Robot</a>.</p>
			        		<p>
			        			<a href="./projects/cs229_poster.pdf">[poster]</a> &nbsp; 
			        			<a href="https://github.com/laoreja/CS229-project-Robot-or-human">[code]</a> &nbsp; 
			        			<a href="./projects/cs229_report.pdf">[report]</a>
			        		</p>
						</td>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./projects/cs229_teaser.png" width="100%"></div>
			          	</td>
					</tr> -->
				</tbody>
			</table>
		</div>
	</div>
</div>	
<div class="space"></div>
<div class="space"></div>
<div class="space"></div>



	<!--/contact-->
<!-- 	<div class="footer">
		<div class="copy">
			<div class="container">
	   </div>
	</div> -->
   <script type="text/javascript">
		$(document).ready(function() {
			/*
			var defaults = {
	  			containerID: 'toTop', // fading element id
				containerHoverID: 'toTopHover', // fading element hover id
				scrollSpeed: 1200,
				easingType: 'linear' 
	 		};
			*/
			
			$().UItoTop({ easingType: 'easeOutQuart' });
			
		});
	</script>
	<a href="#" id="toTop" style="display: block;"><span id="toTopHover"></span><span id="toTopHover"></span> <span id="toTopHover" style="opacity: 1;"> </span></a>
</div>
</body>
</html>
