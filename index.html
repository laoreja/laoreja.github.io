<!DOCTYPE html>
<html>
<head>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-88380739-1', 'auto');
      ga('send', 'pageview');
    </script>	

    <!-- GoStats JavaScript Based Code -->
	<script type="text/javascript" src="https://ssl.gostats.com/js/counter.js"></script>
	<script type="text/javascript">_gos='monster.gostats.com';_goa=488062;_got=5;_goi=1;_gol='website page counter';_GoStatsRun();
	</script>
	<noscript>
		<a target="_blank" title="website page counter" href="http://gostats.com"><img alt="website page counter" src="https://ssl.gostats.com/bin/count/a_488062/t_5/i_1/ssl_monster.gostats.com/counter.png" style="border-width:0" />
		</a>
	</noscript>
<!-- End GoStats JavaScript Based Code -->

<title>Xiuye Gu's Home</title>
<!-- for-mobile-apps -->
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="keywords" content="Xiuye Gu, Computer Vision, Machine Learning, Research, Personal Website" />
<script type="application/x-javascript"> addEventListener("load", function() { setTimeout(hideURLbar, 0); }, false);function hideURLbar(){ window.scrollTo(0,1); } </script>
<!-- //for-mobile-apps -->
<link href="css/bootstrap.css" rel="stylesheet" type="text/css" media="all" />
<link href="css/style.css" rel="stylesheet" type="text/css" media="all" />
<!--fonts-->
<link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
<link href='https://fonts.googleapis.com/css?family=Berkshire+Swash' rel='stylesheet' type='text/css'>
<!--/fonts-->
<!-- js -->
<script src="js/jquery.min.js"> </script>
<script src="js/bootstrap.js"></script>
<!-- //js -->
<!-- start-smoth-scrolling -->
<script type="text/javascript" src="js/move-top.js"></script>
<script type="text/javascript" src="js/easing.js"></script>
<script type="text/javascript">
	jQuery(document).ready(function($) {
		$(".scroll").click(function(event){		
			event.preventDefault();
			$('html,body').animate({scrollTop:$(this.hash).offset().top},1000);
		});
	});
</script>
<link rel="shortcut icon" type="image/png" href="images/favicon.png">
<!-- start-smoth-scrolling -->
</head>
<body>
	<div class="header">
		<div class="container">
<!-- 				<div class="logo">
					<a href="#"><h1>Mungo</h1></a>
				</div> -->
		<div class="navigation">
			<nav class="navbar navbar-default">
				<div class="navbar-header">
					  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
						<span class="sr-only">Toggle navigation</span>
						<span class="icon-bar"> </span>
						<span class="icon-bar"> </span>
						<span class="icon-bar"> </span>
					  </button>
					</div>
					<div class="collapse navbar-collapse nav-wil" id="bs-example-navbar-collapse-1">
						<nav class="cl-effect-7" id="cl-effect-7">
					  <ul class="nav navbar-nav">
					  	<li><a href="index.html">Me &amp; Research</a></li>
						<!-- <li class="active"><a class="scroll" href="#home">Home <span class="sr-only">(current)</span></a></li> -->
						
						<!-- <li><a class="scroll" href="#publications">Publications</a></li> -->
						<!-- <li><a class="scroll" href="index.html#research">Research</a></li> -->
						<li><a href="projects.html">Projects</a></li>
						<li><a href="photos.html">Photos</a></li>
						<li><a href="materials/cv_XiuyeGu.pdf">CV</a></li>
						<li><a href="contact.html">Contact</a></li>
						<!-- <li><a class="scroll" href="#contact">Contact</a></li> -->
					  </ul>
					  </nav>
			  <div class="clearfix"> </div>
			</div><!-- /.navbar-collapse -->
		</nav>
	</div>
	<!-- <div class="search-bar">
			<input type="text" value="Search" onFocus="this.value = '';" onBlur="if (this.value == '') {this.value = 'Search';}">
			<input type="submit" value="">
	   </div> -->
	<div class="clearfix"> </div>
	</div>
</div>


<!-- COMMON HEAD END -->


<div class="container">
	<div class="head-section text-center">
		<div class="space"></div>
		<h2>XIUYE GU</h2>
		<span> </span>
	</div>

	<div class="my_photo me">
		<img class="my_img" src="images/me_200.png">
	</div>
	<div class="self_intro me">
		<p>
			I am a senior at Zhejiang University, majoring in computer science and an undergraduate member of State Key Lab of CAD &amp; CG, Zhejiang University, under the supervision of <a href="http://www.cad.zju.edu.cn/home/dengcai/">Prof. Deng Cai</a>. In summer 2016, I spent a very happy time working under the guidance of <a href="http://www.cs.ucdavis.edu/~yjlee/">Prof. Yong Jae Lee</a> as a research intern, at UC Davis.
		</p>
		<p style="margin-bottom:1.5em">
			I am interested in Computer Vision and Machine Learning. I want to solve high level vision problems, and strive to gain a deeper understanding of Deep Learning. Besides, I am also interested in Approximate Nearest Neighbor Search, in the field of Information Retrieval.
		</p>
	</div>
</div>


<div class="container"><hr style="border-top: 1px solid #D8D8D8"></div>
<div class="space"></div>


<div class="publications" id="research">
	<div class="container">
		<div class="head-section text-center">
			<h2>Research</h2>
			<span> </span>
		</div>
		<div class="projects_table" id="research_table">
        	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
				<tbody>
					<tr>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./images/horse_new.png" width="100%"></div>
			          	</td>
						<td valign="top" width="75%">
			        		<p><papertitle>Maheen Rashid, <b>Xiuye Gu</b>, Yong Jae Lee. <em>Interspecies Knowledge Transfer for Facial Keypoint Detection.</em> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. (Under review).</papertitle></p>
			        		<br>
			        		
			        		<ul>
								<!-- <li><p><b>Background:</b> Detecting pain in animals is important because pain can be indicative of disease and affects animal well being. A system that automatically detects pain would improve animal welfare. Interestingly, research in veterinary science has found that horses, mice, and sheep exhibit facial expressions of pain. For automatic detection of facial expressions, a critical step is facial keypoint detection. In this paper, we tackle the problem of facial keypoint detection for animals.</p></li> -->
								<li><p><b>Motivation:</b> A system that automatically detects pain would improve animal welfare. Interestingly, research in veterinary science has found that many mammals exhibit facial expressions of pain. For automatic detection of facial expressions, a critical step is facial keypoint detection. Directly finetuning a network trained to detect keypoints on human faces to animal faces is sub-optimal, since human and animal faces can look quite different. So we propose to first adapt the animal images to the pre-trained human detection network by correcting for the differences in animal and human face shape. Our reasoning is that this explicit adaptation handles the structural difference, and the fine-tuning can handle the style (texture) difference, because the pre-trained human facial keypoint detection network works better on more human-like animals (cats, monkeys) than on horses and sheep, which are much more different from human faces.</p></li>
								<li><p><b>Approach:</b> We first find the nearest human neighbors for each animal image using an unsupervised shape matching method. We use these matches to train a thin plate spline warping network to warp each animal face to look more human-like. The warping network is then jointly finetuned with a pre-trained human facial keypoint detection network using an animal dataset.</p></li>
								<li><p><b>Brief results:</b> We demonstrate state-of-the-art results on both horse and sheep facial keypoint detection, and significant improvement over simple finetuning especially when training data is scarce.</p></li>
								<li><p><b>Implication:</b> Our reasoning and our network structure can be further generalized to improve the performance of knowledge transfer (instead of directly fine-tuning) between two similar tasks: one is mature with large dataset, the other is a new task with limited data.</p></li>
								<li><p><b>Miscellaneous:</b> We present a new dataset with 3900 horse facial images with facial keypoint annotations. I also implemented a facial landmark annotation tool in Python<!-- , which supports keyboard shortcuts. In this way, we can annotate images more quickly -->. <a href="https://github.com/laoreja/facial-landmark-annotation-tools">[code]</a> <a href="images/annotation.png">[screenshot of the tool]</a></p></li>
			        		</ul>	

			        			<!-- Our project aims to detect animal facial key points under the constraint of a small dataset. In the near future, our system can be used to detect animal expressions, such as pain, to improve animal welfare. Besides, we built a new dataset of 3900 images with horse face and facial key point annotations. The research paper has been submitted to <b>CVPR 2017</b>. Dataset will be publicly shared upon acceptance of our paper.</p> -->
			        	
						</td>
					</tr>
					<tr>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./images/revisit_new.png" width="100%"></div>
			          	</td>
						<td valign="top" width="75%">
							<p>
							<papertitle><b>Xiuye Gu*</b>, Chaoqi Wang*, Cong Fu, Deng Cai. <em>A Revisit on Binary Code Learning for Large-scale Content Based Image Retrieval.</em> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. (Under review). *equal contribution.</papertitle>
							</p>
							<br>
							<ul>
								<!-- <li><p><b>Background:</b> There is a growing trend in using deep hashing methods for content based image retrieval (<b>CBIR</b>) systems, where hashing functions and binary codes are learnt using deep convolutional neural networks (<b>DCNN</b>) and then the binary codes are used for approximate nearest neighbor search (<b>ANNS</b>). All the existing deep hashing papers report that empirical evaluations prove their methods' superior performance over the state-of-the-art conventional supervised and unsupervised hashing algorithms.However, we notice that these deep hashing papers have some common insufficiencies in empirically verifying the effectiveness of their methods and thus their conclusions are dubious.</p>
								</li> -->
								<li><p><b>Motivation:</b> There is a growing trend in using deep hashing methods for content based image retrieval (<b>CBIR</b>) systems. However, we notice that these deep hashing papers have some common insufficiencies in empirically verifying the effectiveness of their methods and thus their conclusions are dubious. So, (1) We want to empirically prove these insufficiencies and then revised the setting. Under the revised setting, we can get a more objective evaluation of the performance of these deep hashing. (2) Beyond the submitted paper, we want to prove one thing more cogently in the near future: fully supervised hashing methods are misleading. <!-- Our reasoning is that deep learning does a great job in classification tasks; encoding the classification results could outperform these supervised hashing methods. --></p></li>
								<li><p><b>Experiments:</b> <!-- In order to make fair comparisons among different approaches, all the deep hashing models in each comparision are retrained on the same pre-trained Resnet classification model (34 layers for the datasets CIFAR-10 and MNIST, 50 layers for dataset ILSVRC-15). </p>--> To prove the small datasets are inappropriate, we compare the deep hashing methods with a simple binary encoding scheme, Classification Coding (<b>CC</b>), derived from classification models<!-- (for each image, we use a c-bit-long code to represent it, where c is the number of classes. Each bit in the code uniquely represents a class) --> on 10%-supervised CIFAR-10 and MNIST (two most commonly used datasets in deep hashing papers). <p>To prove the fully supervised hashing are inappropriate, we compare the deep hashing methods (<a href="https://arxiv.org/abs/1507.00101">SSDH</a>, <a href="http://www.iis.sinica.edu.tw/~kevinlin311.tw/cvprw15.pdf">DLBHC</a>, <a href="http://www.ijcai.org/Proceedings/16/Papers/245.pdf">DPSH</a>) with a 16-bit-long Classification Random Code (<b>CRC</b>) <!-- (each class is randomly mapped to a number ranging from 0 to 2^16 - 1, --> according to <a href="http://www-bcf.usc.edu/~gareth/research/picts.pdf"><em>The error coding method and picts</em></a> <!-- More specifically, we encode the base data using ground-truth labels and encode the query data using the predicted labels obtained from the classification model --> on the fully-supervised ILSVRC-15.</p> <p>We then use the large ILSVRC-15 dataset, but regard only 10% of its training dataset as supervised and take time-precision curves as our evaluation metric to compare the state-of-the-art deep hashing methods with <a href="http://www.mit.edu/~andoni/LSH/">LSH</a>, <a href="http://www.cs.unc.edu/~lazebnik/publications/cvpr11_small_code.pdf">ITQ</a> and the <a href="http://www.cs.ubc.ca/research/flann/uploads/FLANN/flann_pami2014.pdf">randomized KD Tree</a> algorithm. </p></li>
								<li><p><b>Brief results:</b> <em>CC</em> parallels with or even ourperforms these deep hashing methods on both CIFAR-10 and MNIST. <em>CRC</em> outperforms these deep hashing methods on fully-supervised ILSVRC-15. With the revised setting, <!-- empirical results reveal that -->the performance of these deep hashing methods are inferior to some non-deep ANNS methods.</p></li>
								<li><p><b>Interesting findings:</b> According to our experiments, the best one of these deep hashing methods is the simplest DLBHC. Some methods cannot converge when the code length is short and some generate binary codes that perform badly for ANNS. Indeed, the pairwise or triplet ranking loss are unsuitable for large-scale datasets, since in a randomly selected small batch, the portion of pairs with same labels are really small. And the multitable version of traditional unsupervised hashing methods perform very well with much less training costs.</p></li>
								<li><p><b>Miscellaneous:</b> We also presented a <a href="http://dengcai.zjulearning.org:8081/Data/ANNS/ANNSData.html">dataset</a> for evaluating semi-supervised ANNS for content based image retrieval.</li>

							</ul>
<!-- 							<p>
							Deep hashing gradually becomes a hot topic in hashing methods and it now forms a separate category of hashing methods. Under a close scrutiny, however, we noticed several common insufficiencies in most state-of-the-art deep hashing methods. Through comprehensive experiments, we empirically proved these insufficiencies and further verified the inferiority of these deep hashing methods. May be deep hashing is a promising future for hashing methods, whereas the current research results cannot prove this point and more efforts are required in this area. The research paper has been submitted to <b>CVPR 2017</b>. --> 
							</p>
						</td>
					</tr>
				</tbody>
			</table>

        	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
				<tbody>
					<tr>
						<td valign="top" width="75%">
			        		<p><papertitle>EFANNA: An Extremely Fast Approximate Nearest Neighbor Search Algorithm Based on kNN Graph</papertitle></p>
			        		<br>
			        		<ul>
			        			<li><p><b>Background:</b> Approximate nearest neighbor (ANN) search is a fundamental problem in many areas of data mining, machine learning and computer vision. The performance of traditional hierarchical structure (tree) based methods decreases as the dimensionality of data grows, while hashing based methods usually lack efficiency in practice. Recently, the graph based methods have drawn considerable attention. <!-- The main idea is that a neighbor of a neighbor is also likely to be a neighbor, which we refer as NN-expansion. These methods construct a k-nearest neighbor (kNN) graph offline. And at online search stage, these methods find candidate neighbors of a query point in some way (e.g., random selection), and then check the neighbors of these candidate neighbors for closer ones iteratively.  -->Despite some promising results, there are mainly two problems with these approaches: (1) These approaches tend to converge to local optima. (2) Constructing a kNN graph is time consuming.</p></li>
			        			<li><p><b>Approach:</b> We find that these two problems can be nicely solved when we provide a good initialization for NN-expansion. In this paper, we propose EFANNA, an extremely fast approximate nearest neighbor search algorithm based on kNN Graph. Efanna nicely combines the advantages of hierarchical structure based methods and nearest-neighbor-graph based methods.</p></li>
			        			<li><p><b>Brief results:</b> Extensive experiments have shown that EFANNA outperforms the state-of-art algorithms both on approximate nearest neighbor search and approximate nearest neighbor graph construction. To the best of our knowledge, EFANNA is the fastest algorithm so far on the above two tasks.</p></li>
			        			<li><p><b>My contribution:</b> I participated in the EFANNA project, implemented several baseline algorithms, and conducted some comparison experiments. <a href="https://github.com/laoreja/fast-kNN-graph-construction-with-LSH-implementation">[some code]</a></li>
			        		</ul>

			        		<!-- <p>According to our comprehensive experiments, EFANNA is the fastest algorithm so far both on approximate nearest neighbor graph construction and approximate nearest neighbor search. A library EFANNA based on this research is released on Github. I participated in the EFANNA project: implemented several baseline algorithms and conducted the comparison experiments. <a href="https://github.com/laoreja/fast-kNN-graph-construction-with-LSH-implementation">[code]</a></p> -->
						</td>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./images/efanna.png" width="100%"></div>
			          	</td>
					</tr>
					<tr>
						<td valign="top" width="75%">
			        		<p><papertitle>License Plate Recognition</papertitle></p>
			        		<br>
			        		<ul>
			        			<li><p><b>Background:</b> Automatic car license plate recognition systems can be very helpful in transportation management systems. However, current work on this topic still cannot reach a very high accuracy.</p></li>
			        			<li><p><b>Approach:</b> In this project, by proposing an iterative segmentation algorithm and critically integrating ideas derived from related research, I successfully achieved an error rate of 4% on License Plate Segementation. To boost the segmentation performance, I incorporated more robust slant and skew correction in the detection stage. I made my system robust when the plates were incomplete, tilted, skewed, blurred by spots, or with poor illumination.<a href="https://github.com/laoreja/LicensePlate">[code]</a></p></li>
			        			<li><p><b>Miscellaneous:</b> I also wrote three literature reviews on license plate detection, segmentation, and character recognition.</p></li>
			        		</ul>
						</td>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./images/plate.png" width="100%"></div>
			          	</td>
					</tr>
				</tbody>
			</table>
		</div>
	</div>
</div>	
<div class="space"></div>



	<!--/contact-->
	<div class="footer">
		<div class="copy">
			<div class="container">
			<p>Copyright &copy; 2016. Xiuye Gu.</p>
	   </div>
	</div>
   <script type="text/javascript">
		$(document).ready(function() {
			/*
			var defaults = {
	  			containerID: 'toTop', // fading element id
				containerHoverID: 'toTopHover', // fading element hover id
				scrollSpeed: 1200,
				easingType: 'linear' 
	 		};
			*/
			
			$().UItoTop({ easingType: 'easeOutQuart' });
			
		});
	</script>
	<a href="#" id="toTop" style="display: block;"><span id="toTopHover"></span><span id="toTopHover"></span> <span id="toTopHover" style="opacity: 1;"> </span></a>
</div>
</body>
</html>
