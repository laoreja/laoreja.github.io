<ul>
								<!-- <li><p><b>Background:</b> There is a growing trend in using deep hashing methods for content based image retrieval (<b>CBIR</b>) systems, where hashing functions and binary codes are learnt using deep convolutional neural networks (<b>DCNN</b>) and then the binary codes are used for approximate nearest neighbor search (<b>ANNS</b>). All the existing deep hashing papers report that empirical evaluations prove their methods' superior performance over the state-of-the-art conventional supervised and unsupervised hashing algorithms.However, we notice that these deep hashing papers have some common insufficiencies in empirically verifying the effectiveness of their methods and thus their conclusions are dubious.</p>
								</li> -->
								<li><p><b>Motivation:</b> There is a growing trend in using deep hashing methods for content based image retrieval (<b>CBIR</b>) systems. However, we notice that these deep hashing papers have some common insufficiencies in empirically verifying the effectiveness of their methods and thus their conclusions are dubious. So, we want to empirically prove these insufficiencies and then revised the setting. Under the revised setting, we can get a more objective evaluation of the performance of these deep hashing. <!-- (2) Beyond the submitted paper, we want to prove one thing more cogently in the near future: fully supervised hashing methods are misleading. --> <!-- Our reasoning is that deep learning does a great job in classification tasks; encoding the classification results could outperform these supervised hashing methods. --></p></li>
								<li><p><b>Experiments:</b> <!-- In order to make fair comparisons among different approaches, all the deep hashing models in each comparision are retrained on the same pre-trained Resnet classification model (34 layers for the datasets CIFAR-10 and MNIST, 50 layers for dataset ILSVRC-15). </p>--> To prove the small datasets are inappropriate, we compare the deep hashing methods with a simple binary encoding scheme, Classification Coding (<b>CC</b>), derived from classification models<!-- (for each image, we use a c-bit-long code to represent it, where c is the number of classes. Each bit in the code uniquely represents a class) --> on 10%-supervised CIFAR-10 and MNIST (two most commonly used datasets in deep hashing papers). <p>To prove the fully supervised hashing are inappropriate, we compare the deep hashing methods (<a href="https://arxiv.org/abs/1507.00101">SSDH</a>, <a href="http://www.iis.sinica.edu.tw/~kevinlin311.tw/cvprw15.pdf">DLBHC</a>, <a href="http://www.ijcai.org/Proceedings/16/Papers/245.pdf">DPSH</a>) with a 16-bit-long Classification Random Code (<b>CRC</b>) <!-- (each class is randomly mapped to a number ranging from 0 to 2^16 - 1, --> according to <a href="http://www-bcf.usc.edu/~gareth/research/picts.pdf"><em>The error coding method and picts</em></a> <!-- More specifically, we encode the base data using ground-truth labels and encode the query data using the predicted labels obtained from the classification model --> on the fully-supervised ILSVRC-15.</p> <p>We then use the large ILSVRC-15 dataset, but regard only 10% of its training dataset as supervised and take time-precision curves as our evaluation metric to compare the state-of-the-art deep hashing methods with <a href="http://www.mit.edu/~andoni/LSH/">LSH</a>, <a href="http://www.cs.unc.edu/~lazebnik/publications/cvpr11_small_code.pdf">ITQ</a> and the <a href="http://www.cs.ubc.ca/research/flann/uploads/FLANN/flann_pami2014.pdf">randomized KD Tree</a> algorithm. </p></li>
								<li><p><b>Brief results:</b> <em>CC</em> parallels with or even ourperforms these deep hashing methods on both CIFAR-10 and MNIST. <em>CRC</em> outperforms these deep hashing methods on fully-supervised ILSVRC-15. With the revised setting, <!-- empirical results reveal that -->the performance of these deep hashing methods are inferior to some non-deep ANNS methods.</p></li>
								<li><p><b>Interesting findings:</b> According to our experiments, the best one of these deep hashing methods is the simplest DLBHC. Some methods cannot converge when the code length is short and some generate binary codes that perform badly for ANNS. Indeed, the pairwise or triplet ranking loss are unsuitable for large-scale datasets, since in a randomly selected small batch, the portion of pairs with same labels are really small. And the multitable version of traditional unsupervised hashing methods perform very well with much less training costs.</p></li>
								<!-- <li><p><b>Miscellaneous:</b> We also presented a <a href="http://dengcai.zjulearning.org:8081/Data/ANNS/ANNSData.html">dataset</a> for evaluating semi-supervised ANNS for content based image retrieval.</li> -->

							</ul>
<!-- 							<p>
							Deep hashing gradually becomes a hot topic in hashing methods and it now forms a separate category of hashing methods. Under a close scrutiny, however, we noticed several common insufficiencies in most state-of-the-art deep hashing methods. Through comprehensive experiments, we empirically proved these insufficiencies and further verified the inferiority of these deep hashing methods. May be deep hashing is a promising future for hashing methods, whereas the current research results cannot prove this point and more efforts are required in this area. The research paper has been submitted to <b>CVPR 2017</b>. --> 



					<tr>
						<td valign="top" width="75%">
			        		<p><papertitle>EFANNA: An Extremely Fast Approximate Nearest Neighbor Search Algorithm Based on kNN Graph</papertitle></p>
			        		<br>
			        		<ul>
			        			<li><p><b>Background:</b> Approximate nearest neighbor (ANN) search is a fundamental problem in many areas of data mining, machine learning and computer vision. The performance of traditional hierarchical structure (tree) based methods decreases as the dimensionality of data grows, while hashing based methods usually lack efficiency in practice. Recently, the graph based methods have drawn considerable attention. <!-- The main idea is that a neighbor of a neighbor is also likely to be a neighbor, which we refer as NN-expansion. These methods construct a k-nearest neighbor (kNN) graph offline. And at online search stage, these methods find candidate neighbors of a query point in some way (e.g., random selection), and then check the neighbors of these candidate neighbors for closer ones iteratively.  -->Despite some promising results, there are mainly two problems with these approaches: (1) These approaches tend to converge to local optima. (2) Constructing a kNN graph is time consuming.</p></li>
			        			<li><p><b>Approach:</b> We find that these two problems can be nicely solved when we provide a good initialization for NN-expansion. In this paper, we propose EFANNA, an extremely fast approximate nearest neighbor search algorithm based on kNN Graph. Efanna nicely combines the advantages of hierarchical structure based methods and nearest-neighbor-graph based methods.</p></li>
			        			<li><p><b>Brief results:</b> Extensive experiments have shown that EFANNA outperforms the state-of-art algorithms both on approximate nearest neighbor search and approximate nearest neighbor graph construction. To the best of our knowledge, EFANNA is the fastest algorithm so far on the above two tasks.</p></li>
			        			<li><p><b>My contribution:</b> I participated in the EFANNA project, implemented several baseline algorithms, and conducted some comparison experiments. <a href="https://github.com/laoreja/fast-kNN-graph-construction-with-LSH-implementation">[some code]</a></li>
			        		</ul>

			        		<!-- <p>According to our comprehensive experiments, EFANNA is the fastest algorithm so far both on approximate nearest neighbor graph construction and approximate nearest neighbor search. A library EFANNA based on this research is released on Github. I participated in the EFANNA project: implemented several baseline algorithms and conducted the comparison experiments. <a href="https://github.com/laoreja/fast-kNN-graph-construction-with-LSH-implementation">[code]</a></p> -->
						</td>
						<td width="25%">
		            		<div class="two" id="ccc_image"><img src="./images/efanna.png" width="100%"></div>
			          	</td>
					</tr>							